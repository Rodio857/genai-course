{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url = \"https://e7f4684c-fd33-4db0-b1d3-268870ecb84d.europe-west3-0.gcp.cloud.qdrant.io:6333\"\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\n",
    "    url=url,\n",
    "    api_key=api_key,\n",
    "    https=True,\n",
    "    timeout=300\n",
    ")\n",
    "client.create_collection(\n",
    "    collection_name=\"book-content\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"book-content\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/fortaleza-digital.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_documents = [page for page in pages if len(page.page_content) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "ids = [str(uuid.uuid4()) for _ in range(len(filtered_documents))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents=filtered_documents[:100], ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to add the vector store in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "<PERSONA>\n",
    "Eres un especialista resolviendo dudas sobre libros de ficción\n",
    "</PERSONA>\n",
    "\n",
    "<TASK>\n",
    "Tu tarea es refrasear la solicitud del usuario para genera una solicitud refraseada.\n",
    "\n",
    "- Puedes corregir los errores gramaticales\n",
    "- Puedes mejorar la semántica y orden léxico de la palabras para un mejor entendimiento\n",
    "</TASK>\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"{user_request}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rephrase_prompt = ChatPromptTemplate([\n",
    "    SystemMessage(content=SYSTEM_PROMPT),\n",
    "    (\"user\", USER_PROMPT)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrase_prompt.invoke({\"user_request\": \"quien es susan fletcher?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = rephrase_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"user_request\": \"quien es susan fletcher\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = rephrase_prompt | llm | RunnableLambda(lambda x: x.content) | vector_store.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"user_request\": \"quien es susan fletcher\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_documents(documents: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join([document.page_content for document in documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "<PERSONA>\n",
    "Eres un especialista resolviendo dudas sobre libros de ficción\n",
    "</PERSONA>\n",
    "\n",
    "<TAREA>\n",
    "Tu tarea es responder la pregunta del usuario.\n",
    "</TAREA>\n",
    "\n",
    "<RESTRICCIONES>\n",
    "- Solo responde la pregunta del usuario tomando como contexto lo provisto en <CONTEXTO>.\n",
    "</RESTRICCIONES>\n",
    "\n",
    "<CONTEXTO>\n",
    "{context}\n",
    "</CONTEXTO>\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "user question: {user_request}\n",
    "rephrased user question: {rephrased_request}\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_prompt = ChatPromptTemplate([\n",
    "    (\"ai\", SYSTEM_PROMPT),\n",
    "    (\"user\", USER_PROMPT)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "simple_chatbot = (\n",
    "    {\n",
    "        \"user_request\": itemgetter(\"user_request\"),\n",
    "        \"rephrased_request\": rephrase_prompt | llm | RunnableLambda(lambda x: x.content)\n",
    "    } \n",
    "    | RunnablePassthrough() \n",
    "    | {\n",
    "        \"user_request\": itemgetter(\"user_request\"),\n",
    "        \"rephrased_request\": itemgetter(\"rephrased_request\"),\n",
    "        \"context\": itemgetter(\"rephrased_request\") | vector_store.as_retriever(search_kwargs={\"k\": 10}) | RunnableLambda(combine_documents),\n",
    "    }\n",
    "    | qa_prompt \n",
    "    | llm\n",
    "    | RunnableLambda(lambda x: x.content)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_chatbot.invoke({\"user_request\": \"¿quién es susan fletcher?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_documents = []\n",
    "\n",
    "for i in range(0, len(filtered_documents), 5):\n",
    "    content = \"\\n\\n\".join([d.page_content for d in filtered_documents[i : i + 5]])\n",
    "    grouped_documents.append(Document(page_content=content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "<PERSONA>\n",
    "Eres un especialista tomando extractos de paginas y haciendo resumenes\n",
    "</PERSONA>\n",
    "\n",
    "<TAREA>\n",
    "Tu tarea es generar un resumen de contenido de paginas de un libro de literatura\n",
    "</TAREA>\n",
    "\n",
    "<CONSIDERACIONES>\n",
    "- Genera un resumen tomando considerando detalles relevantes\n",
    "</CONSIDERACIONES>\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "contenido: {content}\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "summarize_prompt = ChatPromptTemplate([\n",
    "    (\"ai\", SYSTEM_PROMPT),\n",
    "    (\"user\", USER_PROMPT)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_chain = {\"content\": RunnableLambda(lambda x: x.page_content)} | summarize_prompt | llm | RunnableLambda(lambda x: Document(page_content=x.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_docs = summarized_chain.batch(inputs=grouped_documents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"book-summarized\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"book-summarized\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "ids = [str(uuid.uuid4()) for _ in range(len(summarized_docs))]\n",
    "vector_store.add_documents(documents=summarized_docs, ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How we can add the summarized db to create a better chatbot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"book-summarized\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_chatbot = (\n",
    "    {\n",
    "        \"user_request\": itemgetter(\"user_request\"),\n",
    "        \"rephrased_request\": rephrase_prompt | llm | RunnableLambda(lambda x: x.content)\n",
    "    } \n",
    "    | RunnablePassthrough() \n",
    "    | {\n",
    "        \"user_request\": itemgetter(\"user_request\"),\n",
    "        \"rephrased_request\": itemgetter(\"rephrased_request\"),\n",
    "        \"context_1\": itemgetter(\"rephrased_request\") | vector_store.as_retriever(search_kwargs={\"k\": 10}) | RunnableLambda(combine_documents)\n",
    "    }\n",
    "    | qa_prompt \n",
    "    | llm\n",
    "    | RunnableLambda(lambda x: x.content)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
